---
title: python综合
author: zhangxin
tags:
  - private
  - python
date: 2023-08-17 14:01:13
categories: python
---

#### python的数据类型

int,str,set,list,dict,tuple,float

**可变类型**:可以进行修改,修改后物理地址不发生改变,内部元素发生变化,外部对象不变

	-  list
	-  set
	-  dict

**不可变类型:**不可以进行修改,修改后变为一个新的对象,物理地址发生改变,内部元素不可修改

id(object) 可以查看对象的 是否发生改变



#### dict的key

字典的key和value是一一对应的,所以key需要满足哈希算法的,可变的数据类型是不可以当key的

字典的查询,删除,添加的平均时间复杂度都是O(1), 相比列表与元组,性能更优.

python3.6之前的是无序字典

	-  字典底层维护了一张哈希表,哈希表中每一个元素存储了,哈希值hash,键key,值value

python3.7含之后是有序的

	- 两张表 一个空表(enteies)存储哈希值hash键key值value, 一个列表(indices)存储位置信息index
 - 取值的顺序:
   - dict([key])
   - Hash_value = hash(key)  计算键的哈希值
   - index = hash_value&(Len(indices)-1)  
   - entey_index = indices[index]   index指向enteies中的位置
   - value = enteies[entey_index]

字典的平均时间复杂度是O(1),因为字典是通过哈希算法来实现的,哈希算法不可避免的问题就是hash冲突,python字典发生哈希冲突时,会向下寻找空余位置,直到找到位置.如果在计算key的hash的值时,如果一直找不到空余位置,则字典的时间复杂度就变成了O(n)了



#### 哈希算法

哈希算法又称摘要算法,作用时对任意输入数据进行计算,得到一个固定长度的输出摘要

哈希算法的特点:

- 相同的输入一定的到相同的输出
- 不同的输入大概率得到不同的输出

哈希算法的目的就是验证原始数据是否被篡改



#### python字典哈希冲突问题

- python字典检查两个值是否相等,是比较两个值的哈希值是否相等

- 具有不同值的对象也有可能hash值一样,比如:hash(5)和hash(5.0) 称为哈希冲突

- 哈希冲突的解决办法:

  - 开放寻址法:
    - 从发生碰撞的单元起,按照一定的顺序,从哈希表中寻找一个空闲单元,存放发生碰撞的元素,这个空闲的单元又称为空白单元,或开放单元
    - 开放寻址法现象成一个找车位的问题,如果当前车位有车了就继续往前走,去找下一个空的停车位
    - 方法1: 线性探测法,顺序查找每一个空位,直到找到空位,每次步长为1
    - 方法2: 二次探查,在表的左右位置根据一定的步长进行跳跃探索,每次步长为n
    - 方法3: 伪随机探测,根据公式生成一个随机数,步长以这个随机数为准进行探测
  - 再哈希法:
    - 就是换个哈希函数继续计算,可能还会造成冲突,多准备几个哈希函数
  - 链地址法:
    - 就是把哈希值相同的值放到同一个链表再挨个查,优点是不同的哈希值不会冲突

  - 公共溢出区:
    - 把所有冲突的放在一个特定的溢出区,去那里找



#### 装饰器

实质上也是一个闭包函数,也是一个嵌套函数

作用:

- 在不改变原函数的情况下,对已有函数进行额外的功能扩展

条件:

- 不修改已有函数的源代码
- 不修改已有函数的调用方式
- 给已有函数增加额外的功能

与闭包的区别:

- 参数有且只有一个,并且是函数类型



#### 闭包

闭包就是能够读取其他函数内部变量的函数

作用:

- 保存外部函数的变量,不会随着外部函数调用而销毁

条件:

- 函数嵌套
- 内部函数必须使用了外部函数的变量或参数
- 外部函数返回内部函数,这个使用了外部函数变量的内部函数称为闭包



#### 函数与方法的区别

- 函数是独立的代码块,用于完成独立的任务
- 方法是类中的函数,用于描述类的行为



#### 对象是什么

- 地址 id()
- 类型 type()
- 值 value()



#### 类的继承顺序

- mro()算法



#### GC垃圾回收

引用计数是python的必需功能,分代回收可选(gc.disable 禁用分代回收),gc.collect()手动触发对象回收

- 引用计数
  - 如果没有变量引用某一对象,这个对象就会被回收,python中的每个变量都是对对象的引用,而不是对象本身
  - 核心概念: 变量是指向一个对象的指针,有n个变量指向一个对象,那么该对象的引用计数则为n,又称该对象有n个引用
  - id(变量名)可以查看变量指向的对象的地址, sys.getrefcount(object) 查看引用计数,但是当调用sys.getrefcount()时会临时增加一次引用
  - 缺点: 循环引用,线程锁定以及额外内存和性能开销,循环引用问题会造成内存泄漏
- 分代回收
  - 专门解决循环引用的问题

​	

引用计数实时作用, 循环引用的回收是定期运行的,垃圾回收器将container对象分为三代,每个新对象都从第一代开始,如果一个对象在一个垃圾回收轮次中幸存下来,它将移至较旧(更高)的一代,较低代的回收效率高于较高代,因为大多数新对象往往会被先销毁,这样分代回收的策略能提高性能并减少垃圾回收带来的暂停时间

每一代都有一个独立的计数器和阈值,计数器存储上次收集以来的对象分配数减去释放数的差值,每次分配新的对象容器对象时,cPython都会检查第0代的计数器是否超过阈值(通过gc.get_count()获得三代对象计数器存储的数值),如果超过阈值,python将触发垃圾回收,



#### python对象的生命周期和方法

__ new __ 创建对象

__ init __ 初始化对象

__ del __ 回收、删除对象



#### GIL锁

GIL全局解释器锁,是一个互斥锁,锁是python解释器的而不是python本身的,防止多线程同时执行python的字节码,防止多线程同时访问python对象.GIL锁用来保护指向当前进程状态的指针.



多个线程同时对一个数据进行增加或减少操作,可能导致内存泄漏(引发数据不一致)



一个对象一把锁带来的问题:

- 死锁: 线程之间相互竞争抢锁的资源
- 反复获取和解释锁导致性能降低



GIL锁规则: 任何python字节码的执行都需要获取解释器锁,这样可以防止死锁,并且带来的性能开销不大,但这实际上使所有受cpu约束的python程序(cpu密集型)都是单线程



线程释放GIL锁的两种情况:

- 遇到IO操作
  - 发送一个http请求等,等待响应
  - 当前执行的线程释放后,不再参与锁的抢夺
- time tick到期
  - time tick规定了线程最长执行时间,超过时间后自动释放GIL锁,间隔大致15毫秒
  - 当前执行的线程释放后(多数是cpu密集型任务),继续参与锁的抢夺



单核cpu下 cpu的利用率很高

多核cpu下由于GIL锁的全局特性,无法发挥多核的特性,GIL锁会使的多线程任务的效率大大降低

GIL降低了多核的效率,保留的目的是线程执行的安全问题



#### python 多进程 多线程 协程

- 并行
  - 同一时间执行多个任务,这里指的是同一时间执行了多个任务中的1类操作,即实际占用CPU运算资源的操作,所以并行意味着会用到多个CPU
  - 只有使用多进程机制处理的任务才属于真正的并行
- 并发
  - 同一时间处理多个任务,这里是处理而不是执行,因为对于并发来说,同一时间在执行1类操作的只有一个任务,其他的任务都在执行2类任务,即等待(基本是硬盘或网络I/O),所以并发不需要用到多个CPU

- 同步
  - 一个函数执行结束等待返回结果后再去执行下一个函数
- 异步
  - 异步的本质就是同一时间处理多个任务,且这多个任务可以交替执行,而协程就是执行异步调用的一种方式
- 阻塞
  - 阻塞调用是指调用结果返回之前,当前线程会被挂起,一直处于等待消息通知,不能够执行其他业务,等待当前函数返回

- 非阻塞
  - 非阻塞调用指在不能立刻返回结果之前也会立即返回,同时该函数不会阻塞当前线程
- 多进程
  - 对于cpu密集型任务,采用更多的计算单元,从而到达在单位时间内进行更多的计算操作
  - 多进程采用python标准库multiprocessing.Pool(process).map(function,iterable),process为使用的进程数,默认是os.cpu_count()的返回值,即cpu数量
  - map会阻塞主程序,如果不需要阻塞主程序可以使用map_async(),参数和map一样,但是它会异步多进程
  - 如果function需要传入多个参数,可以使用starmap(),参数和map一样,区别在于starmap会将iterable中的每一项拆包作为function的输入参数
  - 如果function需要传入多个参数且不希望主进程阻塞,可以使用starmap_async()

- 多线程
  - 同时执行多个不同程序
- 协程
  - 对于I/O密集型任务,只需要使各个等待I/O的实际重叠,就能够加速任务执行,使用多线程或协程即可对I/O密集型的任务进行加速
  - 之所以可以使用多线程对I/O密集型任务进行加速,是因为python程序在进行I/O操作时会释放GIL,因此当某一个线程在等待I/O时,可以转而执行下一个线程
  - asynico是python标准库中使用协程来异步执行程序从而实现并发的库
  - async和await是python3.5引入,asyncio.run()是3.7引入的
  - async def 定义且tonguereturn返回值的函数为协程函数,协程函数的返回对象称为协程对象,是可等待对象的一种
  - 通过async def定义且通过 yield返回值的函数称为协程生成器,可以通过 async for 来进行迭代
  - 能够被await驱动的对象类型称为awaitable即可等待对象.python标准库中定义了特殊方法 __ await __()的对象都是可等待对象;主要有三类:coroutine,Task,Future.  corotine就是协程函数返回的对象;Task为asycio.creat_task(coroutine)的返回对象
  - 不使用await或asynico.run驱动,而是直接调用协程函数,则协程函数会返回一个协程对象
  - 协程的执行过程基本和多线程类似,但是协程之间的切换会更紧凑一些. 协程本质上在用户程序和低层线程之间搭建了一个管道,从而把协程调度的工作委派了事件循环.我们确保代码中没有阻塞的代码,事件循环会处理并发
  - 协程相比多线程的优势是没有线程切换的开销,因为协程都是运行在一个线程上,不存在由于多线程的“竞争机制”导致的同时写变量冲突等问题,因此,在协程中对于共享资源不需要加锁



#### 多线程和多进程的使用场景

1. 多线程 多线程是一种并发编程的方式，它可以在同一个进程中创建多个线程，每个线程都可以独立执行任务。多线程可以共享进程的内存空间，从而可以方便地共享数据和资源。多线程适用于 I/O 密集型任务，例如网络通信、文件读写等，以及需要共享数据和资源的任务。
2. 多进程 多进程是一种并发编程的方式，它可以在操作系统中创建多个进程，每个进程都可以独立执行任务。多进程可以通过进程间通信来共享数据和资源，但是相比于多线程，进程间的通信成本更高。多进程适用于 CPU 密集型任务，例如计算密集型算法、图像处理等，以及需要隔离和保护数据和资源的任务。



#### 什么是竞争条件?

- 竞争条件是指当多个进程或线程同时访问共享资源时,由于执行顺序不确定或不可预测时,导致最终结果的正确性受到破坏的情况,最终结果可能与预期不符,因为进程或线程之间的相互竞争导致了不确定的执行顺序
- 可以加锁避免



#### with与上下文管理器

- 上下文管理器
  - 上下文管理器定义执行with语句时要建立运行上下文,负责执行with语句块上下文中的进入与退出操作
  - __ enter __ 方法在语句执行之前进入运行时上下文
  - __ exit __ 方法在语句执行完成后从运行时上下文退出
  - 实际应用中,__ enter __一般用于资源分配,如打开文件,链接数据库,获取线程锁, exit 一般用于资源的释放,如关闭文件,关闭数据库连接,释放线程锁

- 使用with试下一个打开关闭数据库的上下文管理器

```python
class DB(object):

    def __init__(self, host, port, user, pwd, database):
        setting = {
            "host": host,
            "port": port,
            "user": user,
            "password": pwd,
            "database": database,
        }
        # 创建数据库连接
        self.dbconn = pymysql.connect(**setting, local_infile=1)
        # 判断数据库链接是否有效
        while True:
            try:
                self.dbconn.ping()
                break
            except OperationalError:
                self.dbconn.ping(True)
        # 创建字典型游标(返回的数据是字典类型)
        self.dbcur = self.dbconn.cursor(cursor=pymysql.cursors.DictCursor)

    # __enter__() 和 __exit__() 是with关键字调用的必须方法
    # with本质上就是调用对象的enter和exit方法
    def __enter__(self):
        # 返回游标
        return self.dbcur

    def __exit__(self, exc_type, exc_value, exc_trace):
        # 提交事务
        self.dbconn.commit()

        # 关闭游标
        self.dbcur.close()

        # 关闭数据库连接
        self.dbconn.close()
```



#### Rabbitmq和Kafka的区别

 rabbitmq

- 消息确认：rabbitmq采用ack机制，确保消息到达消费者。消费者接收并处理消息后需要手动发送ack确认消息已经完成处理
- 消息持久化：rabbitmq可以将消息存储到硬盘上，以便在服务器故障时恢复丢失的数据
- 集群管理：rabbitmq提供了一个可靠的集群管理机制，包括主备模式，故障转移等方式来确保高可用性
- 缺点：rabbitmq的处理数据较慢，因为它依赖磁盘I/O

kafka

- 消息确认：kafka使用分布式提交日志机制，消费者接收到消息后会自动提交确认标记（offset），不需要手动确认
- 消息持久化：kafka将所有消息都存储在磁盘上，因此可以支持大量的消息，并且具有更好的读写性能
- 集群管理：kafka的分布式消息传输设计非常出色，可以轻松扩展以处理海量数据，同时支持动态扩容。
- 缺点：kafka在多点写入时（producer端）性能不如rabblitmq

综上所述，rabbitmq在可靠性和管理方面表现更好，而kafka则具有更好的性能和扩展性。因此，选择哪个系统需要根据具体的业务需求来决定。



#### 传输很长的数据时如何保证数据完整的传输到指定地方

- TCP协议：tcp协议提供可靠的数据传输保证，它会自动进行数据分段、校验和等操作，并发送确认消息来确保数据的可靠性传输，因此，在数据量大且需要保证可靠性的情况下，可以使用tcp协议来进行数据传输
- HTTP分块传输编码：http分块传输编码是一种将数据分成多个块进行传输的方式，每个块都有一个长度标识符和内容，在接收端需要将所有的块合并成一个完整的数据，通过这种方式可以避免一次性传输过大的数据导致连接中断的问题
- 文件压缩：将要传输的数据进行压缩处理，可以减少数据传输的大小，降低数据传输的时间和带宽消耗，并且能提高数据传输的安全性
- 数据摘要算法：在数据传输开始前对要纯属的数据进行摘要计算（如MD5或者SHA），然后计算出来的结果与要传输的数据一起传输。在接收端，再重新计算一次数据摘要，将计算出来的的结果与传输的摘要进行对比，如果一致所用数据完整
- 保证分段数据的顺序
  - tcp协议是一种基于流的协议，会按照发送顺序保证数据的接收顺序，因此，在需要保证数据顺序的情况下，可以使用TCP协议进行传输
  - 消息队列可以保证消息的顺序性和可靠性，适合处理高并发。分布式系统等场景，在消息队列中，每个生产者发送的消息都会被放入队列中，并按照发送顺序进行排序，消费者从队列中取出消息时，会按照发送顺序进行消费
  - 在传输过程中，可以给每个数据包添加一个唯一的序号，然后在接收端根据序号进行排序，从而保证数据的顺序性



#### 布隆过滤器的原理和使用场景

https://juejin.cn/post/7036221560117526541

布隆过滤器是一种快速，高效的数据结构，用于判断一个元素是否存在于一个集合中。其基本原理是将每个元素通过多个相互独立的哈希函数映射成位数组中的多个位置，并将这些位置标记成1。在判断一个元素是否存在，我们只需要检查该元素对应的各个位置是否都被标记为1即可。

布隆过滤器的优点在于空间和时间效率非常高，它可以使用极少的内存来存储海量的数据，并且在插入和查询操作中具有O(k)的时间复杂度，其中K是哈希函数的个数。此外，布隆过滤器还支持动态添加和删除元素，但是删除操作可能会导致误判率的上升。

应用场景：

- 网络爬虫：在爬取网页时，可以使用布隆过滤器来避免重复抓取同一个页面
- 缓存系统：在缓存系统中，我们可以使用布隆过滤器来判断一个请求是否需要从数据库中查询，以提高缓存命中率
- 防止DDOS攻击：在网路安全领域中，布隆过滤器可以用于快速判断一个IP地址是否在黑名单中，以防止DDos攻击等恶意行为
- 数据库查询优化：在数据库查询中，布隆过滤器可以用于排除不存在的记录，从而提高查询效率



#### python抽象类和接口类的区别

- 抽象类
  - 抽象类是不能被实例化的类,它定义了一组方法的名称和参数,但没有具体的实现,子类需要继承这个抽象类,并且必须实现其中定义的所有方法,否则子类也会被认为是抽象类,抽象类通常用于定于一些公共的接口和行为,让子类去实现具体功能.在python中,可以使用abc模块来定义抽象类
- 接口类
  - 接口类也同样定义了一组的名称和参数,但是其特点是所有方法都是空的,没有任何实现.通过继承接口类并实现其中定义的方法,可以达到实现某种特定的功能的目的.在python中,并没有专门的接口类语法,但是可以使用抽象类来实现接口类的功能.

- 区别
  - 抽象类中可以包含非抽象方法(即有实现的方法),而接口类中只包含没有实现的方法
  - 抽象类可以维护一个状态,而接口类则不可以
  - 子类可以使用多继承方式同时继承多个抽象类,但是接口类不支持多继承.

总之,抽象类和接口类都是定义一组方法的规范,它们的区别在于抽象类可以包含有实现的方法,而接口类只包含没有实现的方法.根据具体的需求和设计,选择使用哪种方式来定义抽象规范



#### 元类

- type通常用来判断对象的类型,它最大的用涂是用来动态创建类,当python扫描到class语法的时候,就会调用type函数来创建类

- type如何创建类

  type需要接收三个参数

  - 类的名称,不指定也要传入空字符串

  - 父类的名称,需要tuple的形式传入,没有也需要传入空的tuple,默认继承object

  - 绑定方法或者属性,用dict的形式传入

    ```python
    # 准备一个基类
    class BaseClass:
      def talk(self):
        print("我是人类")
        
    # 准备一个方法
    def say(self):
      print("hello")
      
    # 使用type创建User类
    User = type(“user”,(BaseClass,),{"name":"user": "say":say})
    ```

- 理解什么是元类

  - 元类是创建类的模版
  - type是python是在背后所有创建类的元类,object也是由type创建的,type也是type创建的
  - 一个实例的类型是类,一个类的类型是元类,一个元类的类型是type









#### Redis在项目过程中做了哪些事?

#### Celery定时机制是什么

#### 项目遇到过什么性能瓶颈?

#### 组内分工

#### 解耦体现在什么方面

#### 项目做了什么模块

#### 项目介绍,用了什么技术栈

#### 单元测试覆盖率

#### 百万级别的数据如何高效存储
