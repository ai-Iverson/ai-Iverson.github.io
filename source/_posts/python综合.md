---
title: python综合
author: zhangxin
tags:
  - private
  - python
date: 2023-08-17 14:01:13
categories: python
---

## Python综合

#### python的数据类型

int,str,set,list,dict,tuple,float

**可变类型**:可以进行修改,修改后物理地址不发生改变,内部元素发生变化,外部对象不变

	-  list
	-  set
	-  dict

**不可变类型:**不可以进行修改,修改后变为一个新的对象,物理地址发生改变,内部元素不可修改

id(object) 可以查看对象的 是否发生改变



#### dict的key

字典的key和value是一一对应的,所以key需要满足哈希算法的,可变的数据类型是不可以当key的

字典的查询,删除,添加的平均时间复杂度都是O(1), 相比列表与元组,性能更优.

python3.6之前的是无序字典

	-  字典底层维护了一张哈希表,哈希表中每一个元素存储了,哈希值hash,键key,值value

python3.7含之后是有序的

	- 两张表 一个空表(enteies)存储哈希值hash键key值value, 一个列表(indices)存储位置信息index
 - 取值的顺序:
   - dict([key])
   - Hash_value = hash(key)  计算键的哈希值
   - index = hash_value&(Len(indices)-1)  
   - entey_index = indices[index]   index指向enteies中的位置
   - value = enteies[entey_index]

字典的平均时间复杂度是O(1),因为字典是通过哈希算法来实现的,哈希算法不可避免的问题就是hash冲突,python字典发生哈希冲突时,会向下寻找空余位置,直到找到位置.如果在计算key的hash的值时,如果一直找不到空余位置,则字典的时间复杂度就变成了O(n)了



#### 哈希算法

哈希算法又称摘要算法,作用时对任意输入数据进行计算,得到一个固定长度的输出摘要

哈希算法的特点:

- 相同的输入一定的到相同的输出
- 不同的输入大概率得到不同的输出

哈希算法的目的就是验证原始数据是否被篡改



#### python字典哈希冲突问题

- python字典检查两个值是否相等,是比较两个值的哈希值是否相等

- 具有不同值的对象也有可能hash值一样,比如:hash(5)和hash(5.0) 称为哈希冲突

- 哈希冲突的解决办法:

  - 开放寻址法:
    - 从发生碰撞的单元起,按照一定的顺序,从哈希表中寻找一个空闲单元,存放发生碰撞的元素,这个空闲的单元又称为空白单元,或开放单元
    - 开放寻址法现象成一个找车位的问题,如果当前车位有车了就继续往前走,去找下一个空的停车位
    - 方法1: 线性探测法,顺序查找每一个空位,直到找到空位,每次步长为1
    - 方法2: 二次探查,在表的左右位置根据一定的步长进行跳跃探索,每次步长为n
    - 方法3: 伪随机探测,根据公式生成一个随机数,步长以这个随机数为准进行探测
  - 再哈希法:
    - 就是换个哈希函数继续计算,可能还会造成冲突,多准备几个哈希函数
  - 链地址法:
    - 就是把哈希值相同的值放到同一个链表再挨个查,优点是不同的哈希值不会冲突

  - 公共溢出区:
    - 把所有冲突的放在一个特定的溢出区,去那里找



#### 装饰器

实质上也是一个闭包函数,也是一个嵌套函数

作用:

- 在不改变原函数的情况下,对已有函数进行额外的功能扩展

条件:

- 不修改已有函数的源代码
- 不修改已有函数的调用方式
- 给已有函数增加额外的功能

与闭包的区别:

- 参数有且只有一个,并且是函数类型



#### 闭包

闭包就是能够读取其他函数内部变量的函数

作用:

- 保存外部函数的变量,不会随着外部函数调用而销毁

条件:

- 函数嵌套
- 内部函数必须使用了外部函数的变量或参数
- 外部函数返回内部函数,这个使用了外部函数变量的内部函数称为闭包



#### 函数与方法的区别

- 函数是独立的代码块,用于完成独立的任务
- 方法是类中的函数,用于描述类的行为



#### 对象是什么

- 地址 id()
- 类型 type()
- 值 value()



#### 类的继承顺序

- mro()算法



#### GC垃圾回收

引用计数是python的必需功能,分代回收可选(gc.disable 禁用分代回收),gc.collect()手动触发对象回收

- 引用计数
  - 如果没有变量引用某一对象,这个对象就会被回收,python中的每个变量都是对对象的引用,而不是对象本身
  - 核心概念: 变量是指向一个对象的指针,有n个变量指向一个对象,那么该对象的引用计数则为n,又称该对象有n个引用
  - id(变量名)可以查看变量指向的对象的地址, sys.getrefcount(object) 查看引用计数,但是当调用sys.getrefcount()时会临时增加一次引用
  - 缺点: 循环引用,线程锁定以及额外内存和性能开销,循环引用问题会造成内存泄漏
- 分代回收
  - 专门解决循环引用的问题

​	

引用计数实时作用, 循环引用的回收是定期运行的,垃圾回收器将container对象分为三代,每个新对象都从第一代开始,如果一个对象在一个垃圾回收轮次中幸存下来,它将移至较旧(更高)的一代,较低代的回收效率高于较高代,因为大多数新对象往往会被先销毁,这样分代回收的策略能提高性能并减少垃圾回收带来的暂停时间

每一代都有一个独立的计数器和阈值,计数器存储上次收集以来的对象分配数减去释放数的差值,每次分配新的对象容器对象时,cPython都会检查第0代的计数器是否超过阈值(通过gc.get_count()获得三代对象计数器存储的数值),如果超过阈值,python将触发垃圾回收,



#### python对象的生命周期和方法

__ new __ 创建对象

__ init __ 初始化对象

__ del __ 回收、删除对象



#### GIL锁

GIL全局解释器锁,是一个互斥锁,锁是python解释器的而不是python本身的,防止多线程同时执行python的字节码,防止多线程同时访问python对象.GIL锁用来保护指向当前进程状态的指针.



多个线程同时对一个数据进行增加或减少操作,可能导致内存泄漏(引发数据不一致)



一个对象一把锁带来的问题:

- 死锁: 线程之间相互竞争抢锁的资源
- 反复获取和解释锁导致性能降低



GIL锁规则: 任何python字节码的执行都需要获取解释器锁,这样可以防止死锁,并且带来的性能开销不大,但这实际上使所有受cpu约束的python程序(cpu密集型)都是单线程



线程释放GIL锁的两种情况:

- 遇到IO操作
  - 发送一个http请求等,等待响应
  - 当前执行的线程释放后,不再参与锁的抢夺
- time tick到期
  - time tick规定了线程最长执行时间,超过时间后自动释放GIL锁,间隔大致15毫秒
  - 当前执行的线程释放后(多数是cpu密集型任务),继续参与锁的抢夺



单核cpu下 cpu的利用率很高

多核cpu下由于GIL锁的全局特性,无法发挥多核的特性,GIL锁会使的多线程任务的效率大大降低

GIL降低了多核的效率,保留的目的是线程执行的安全问题



#### python 多进程 多线程 协程

- 并行
  - 同一时间执行多个任务,这里指的是同一时间执行了多个任务中的1类操作,即实际占用CPU运算资源的操作,所以并行意味着会用到多个CPU
  - 只有使用多进程机制处理的任务才属于真正的并行
- 并发
  - 同一时间处理多个任务,这里是处理而不是执行,因为对于并发来说,同一时间在执行1类操作的只有一个任务,其他的任务都在执行2类任务,即等待(基本是硬盘或网络I/O),所以并发不需要用到多个CPU

- 同步
  - 一个函数执行结束等待返回结果后再去执行下一个函数
- 异步
  - 异步的本质就是同一时间处理多个任务,且这多个任务可以交替执行,而协程就是执行异步调用的一种方式
- 阻塞
  - 阻塞调用是指调用结果返回之前,当前线程会被挂起,一直处于等待消息通知,不能够执行其他业务,等待当前函数返回

- 非阻塞
  - 非阻塞调用指在不能立刻返回结果之前也会立即返回,同时该函数不会阻塞当前线程
- 多进程
  - 对于cpu密集型任务,采用更多的计算单元,从而到达在单位时间内进行更多的计算操作
  - 多进程采用python标准库multiprocessing.Pool(process).map(function,iterable),process为使用的进程数,默认是os.cpu_count()的返回值,即cpu数量
  - map会阻塞主程序,如果不需要阻塞主程序可以使用map_async(),参数和map一样,但是它会异步多进程
  - 如果function需要传入多个参数,可以使用starmap(),参数和map一样,区别在于starmap会将iterable中的每一项拆包作为function的输入参数
  - 如果function需要传入多个参数且不希望主进程阻塞,可以使用starmap_async()

- 多线程
  - 同时执行多个不同程序
- 协程
  - 对于I/O密集型任务,只需要使各个等待I/O的实际重叠,就能够加速任务执行,使用多线程或协程即可对I/O密集型的任务进行加速
  - 之所以可以使用多线程对I/O密集型任务进行加速,是因为python程序在进行I/O操作时会释放GIL,因此当某一个线程在等待I/O时,可以转而执行下一个线程
  - asynico是python标准库中使用协程来异步执行程序从而实现并发的库
  - async和await是python3.5引入,asyncio.run()是3.7引入的
  - async def 定义且tonguereturn返回值的函数为协程函数,协程函数的返回对象称为协程对象,是可等待对象的一种
  - 通过async def定义且通过 yield返回值的函数称为协程生成器,可以通过 async for 来进行迭代
  - 能够被await驱动的对象类型称为awaitable即可等待对象.python标准库中定义了特殊方法 __ await __()的对象都是可等待对象;主要有三类:coroutine,Task,Future.  corotine就是协程函数返回的对象;Task为asycio.creat_task(coroutine)的返回对象
  - 不使用await或asynico.run驱动,而是直接调用协程函数,则协程函数会返回一个协程对象
  - 协程的执行过程基本和多线程类似,但是协程之间的切换会更紧凑一些. 协程本质上在用户程序和低层线程之间搭建了一个管道,从而把协程调度的工作委派了事件循环.我们确保代码中没有阻塞的代码,事件循环会处理并发
  - 协程相比多线程的优势是没有线程切换的开销,因为协程都是运行在一个线程上,不存在由于多线程的“竞争机制”导致的同时写变量冲突等问题,因此,在协程中对于共享资源不需要加锁



#### 多线程和多进程的使用场景

1. 多线程 多线程是一种并发编程的方式，它可以在同一个进程中创建多个线程，每个线程都可以独立执行任务。多线程可以共享进程的内存空间，从而可以方便地共享数据和资源。多线程适用于 I/O 密集型任务，例如网络通信、文件读写等，以及需要共享数据和资源的任务。
2. 多进程 多进程是一种并发编程的方式，它可以在操作系统中创建多个进程，每个进程都可以独立执行任务。多进程可以通过进程间通信来共享数据和资源，但是相比于多线程，进程间的通信成本更高。多进程适用于 CPU 密集型任务，例如计算密集型算法、图像处理等，以及需要隔离和保护数据和资源的任务。



#### 什么是竞争条件?

- 竞争条件是指当多个进程或线程同时访问共享资源时,由于执行顺序不确定或不可预测时,导致最终结果的正确性受到破坏的情况,最终结果可能与预期不符,因为进程或线程之间的相互竞争导致了不确定的执行顺序
- 可以加锁避免



#### with与上下文管理器

- 上下文管理器
  - 上下文管理器定义执行with语句时要建立运行上下文,负责执行with语句块上下文中的进入与退出操作
  - __ enter __ 方法在语句执行之前进入运行时上下文
  - __ exit __ 方法在语句执行完成后从运行时上下文退出
  - 实际应用中,__ enter __一般用于资源分配,如打开文件,链接数据库,获取线程锁, exit 一般用于资源的释放,如关闭文件,关闭数据库连接,释放线程锁

- 使用with试下一个打开关闭数据库的上下文管理器

```python
class DB(object):

    def __init__(self, host, port, user, pwd, database):
        setting = {
            "host": host,
            "port": port,
            "user": user,
            "password": pwd,
            "database": database,
        }
        # 创建数据库连接
        self.dbconn = pymysql.connect(**setting, local_infile=1)
        # 判断数据库链接是否有效
        while True:
            try:
                self.dbconn.ping()
                break
            except OperationalError:
                self.dbconn.ping(True)
        # 创建字典型游标(返回的数据是字典类型)
        self.dbcur = self.dbconn.cursor(cursor=pymysql.cursors.DictCursor)

    # __enter__() 和 __exit__() 是with关键字调用的必须方法
    # with本质上就是调用对象的enter和exit方法
    def __enter__(self):
        # 返回游标
        return self.dbcur

    def __exit__(self, exc_type, exc_value, exc_trace):
        # 提交事务
        self.dbconn.commit()

        # 关闭游标
        self.dbcur.close()

        # 关闭数据库连接
        self.dbconn.close()
```



#### Rabbitmq和Kafka的区别

 rabbitmq

- 消息确认：rabbitmq采用ack机制，确保消息到达消费者。消费者接收并处理消息后需要手动发送ack确认消息已经完成处理
- 消息持久化：rabbitmq可以将消息存储到硬盘上，以便在服务器故障时恢复丢失的数据
- 集群管理：rabbitmq提供了一个可靠的集群管理机制，包括主备模式，故障转移等方式来确保高可用性
- 缺点：rabbitmq的处理数据较慢，因为它依赖磁盘I/O

kafka

- 消息确认：kafka使用分布式提交日志机制，消费者接收到消息后会自动提交确认标记（offset），不需要手动确认
- 消息持久化：kafka将所有消息都存储在磁盘上，因此可以支持大量的消息，并且具有更好的读写性能
- 集群管理：kafka的分布式消息传输设计非常出色，可以轻松扩展以处理海量数据，同时支持动态扩容。
- 缺点：kafka在多点写入时（producer端）性能不如rabblitmq

综上所述，rabbitmq在可靠性和管理方面表现更好，而kafka则具有更好的性能和扩展性。因此，选择哪个系统需要根据具体的业务需求来决定。



#### 传输很长的数据时如何保证数据完整的传输到指定地方

- TCP协议：tcp协议提供可靠的数据传输保证，它会自动进行数据分段、校验和等操作，并发送确认消息来确保数据的可靠性传输，因此，在数据量大且需要保证可靠性的情况下，可以使用tcp协议来进行数据传输
- HTTP分块传输编码：http分块传输编码是一种将数据分成多个块进行传输的方式，每个块都有一个长度标识符和内容，在接收端需要将所有的块合并成一个完整的数据，通过这种方式可以避免一次性传输过大的数据导致连接中断的问题
- 文件压缩：将要传输的数据进行压缩处理，可以减少数据传输的大小，降低数据传输的时间和带宽消耗，并且能提高数据传输的安全性
- 数据摘要算法：在数据传输开始前对要纯属的数据进行摘要计算（如MD5或者SHA），然后计算出来的结果与要传输的数据一起传输。在接收端，再重新计算一次数据摘要，将计算出来的的结果与传输的摘要进行对比，如果一致所用数据完整
- 保证分段数据的顺序
  - tcp协议是一种基于流的协议，会按照发送顺序保证数据的接收顺序，因此，在需要保证数据顺序的情况下，可以使用TCP协议进行传输
  - 消息队列可以保证消息的顺序性和可靠性，适合处理高并发。分布式系统等场景，在消息队列中，每个生产者发送的消息都会被放入队列中，并按照发送顺序进行排序，消费者从队列中取出消息时，会按照发送顺序进行消费
  - 在传输过程中，可以给每个数据包添加一个唯一的序号，然后在接收端根据序号进行排序，从而保证数据的顺序性



#### 布隆过滤器的原理和使用场景

https://juejin.cn/post/7036221560117526541

布隆过滤器是一种快速，高效的数据结构，用于判断一个元素是否存在于一个集合中。其基本原理是将每个元素通过多个相互独立的哈希函数映射成位数组中的多个位置，并将这些位置标记成1。在判断一个元素是否存在，我们只需要检查该元素对应的各个位置是否都被标记为1即可。

布隆过滤器的优点在于空间和时间效率非常高，它可以使用极少的内存来存储海量的数据，并且在插入和查询操作中具有O(k)的时间复杂度，其中K是哈希函数的个数。此外，布隆过滤器还支持动态添加和删除元素，但是删除操作可能会导致误判率的上升。

应用场景：

- 网络爬虫：在爬取网页时，可以使用布隆过滤器来避免重复抓取同一个页面
- 缓存系统：在缓存系统中，我们可以使用布隆过滤器来判断一个请求是否需要从数据库中查询，以提高缓存命中率
- 防止DDOS攻击：在网路安全领域中，布隆过滤器可以用于快速判断一个IP地址是否在黑名单中，以防止DDos攻击等恶意行为
- 数据库查询优化：在数据库查询中，布隆过滤器可以用于排除不存在的记录，从而提高查询效率



#### python抽象类和接口类的区别

- 抽象类
  - 抽象类是不能被实例化的类,它定义了一组方法的名称和参数,但没有具体的实现,子类需要继承这个抽象类,并且必须实现其中定义的所有方法,否则子类也会被认为是抽象类,抽象类通常用于定于一些公共的接口和行为,让子类去实现具体功能.在python中,可以使用abc模块来定义抽象类
- 接口类
  - 接口类也同样定义了一组的名称和参数,但是其特点是所有方法都是空的,没有任何实现.通过继承接口类并实现其中定义的方法,可以达到实现某种特定的功能的目的.在python中,并没有专门的接口类语法,但是可以使用抽象类来实现接口类的功能.

- 区别
  - 抽象类中可以包含非抽象方法(即有实现的方法),而接口类中只包含没有实现的方法
  - 抽象类可以维护一个状态,而接口类则不可以
  - 子类可以使用多继承方式同时继承多个抽象类,但是接口类不支持多继承.

总之,抽象类和接口类都是定义一组方法的规范,它们的区别在于抽象类可以包含有实现的方法,而接口类只包含没有实现的方法.根据具体的需求和设计,选择使用哪种方式来定义抽象规范



#### 元类

- type通常用来判断对象的类型,它最大的用涂是用来动态创建类,当python扫描到class语法的时候,就会调用type函数来创建类

- type如何创建类

  type需要接收三个参数

  - 类的名称,不指定也要传入空字符串

  - 父类的名称,需要tuple的形式传入,没有也需要传入空的tuple,默认继承object

  - 绑定方法或者属性,用dict的形式传入

    ```python
    # 准备一个基类
    class BaseClass:
      def talk(self):
        print("我是人类")
        
    # 准备一个方法
    def say(self):
      print("hello")
      
    # 使用type创建User类
    User = type(“user”,(BaseClass,),{"name":"user": "say":say})
    ```

- 理解什么是元类

  - 元类是创建类的模版
  - type是python是在背后所有创建类的元类,object也是由type创建的,type也是type创建的
  - 一个实例的类型是类,一个类的类型是元类,一个元类的类型是type

#### 分布式锁

- 分布式锁是一种分布式系统中的并发控制，它可以确保在分布式环境中只有一个进程或线程能够访问共享资源，分布式锁通常基于某种可靠的存储系统（如redis）实现，在不同进程或服务器时间协调并管理共享锁
- 分布式锁实现方式通常有两种：基于数据库和基于缓存。基于数据库的实现方式需要使用关系型数据库的行级锁或者悲观锁控制并发访问，但是这种方式由于需要频繁的对数据库进行访问，会给数据库带来较大的性能损耗。而基于缓存的实现方式则需要使用缓存系统的原子加锁操作来实现分布式锁，这种方式相比于基于数据库的方式更加轻量级，但需要注意避免缓存雪崩和缓存穿透的问题。
- 分布式锁在分布式系统中非常重要，它可以解决多个进程或节点之间对共享资源的竞争访问问题，避免了数据不一致和脏读等问题的出现，但是，分布式锁的实现需要注意避免死锁，过期时间的设置以及锁的粒度问题，否则会影响分布式系统的性能和可用性。

###  

## Mysql

https://github.com/caokegege/Interview/blob/master/db/%E6%9C%80%E5%85%A8MySQL%E9%9D%A2%E8%AF%9560%E9%A2%98%E5%92%8C%E7%AD%94%E6%A1%88.md

https://zhuanlan.zhihu.com/p/164511591



#### 数据库同步策略

- 异步复制(mysql默认的同步方式)
  - 在master为slave开通账号密码,ip授权之后,slave可以从master进行数据同步,主要依赖的是master的binlog日志
  - slave会启动两个线程,IO Thread和SQL Thread
  - IO Thread负责从master拉取binlog日志,并写入ralay中继日志
  - SQL Thread负责将relay中继日志中的变更进行更新重放,更新数据来达到跟master保持数据一致的目的
  - 过程中.slave通过IO线程拉取binlog日志,master无需关注是否有slave需要同步,整个复制过程都是异步完成的
  - 优势: 性能好
  - 缺点: 安全性比较差
  - 在某一刻主从之间的数据差异可能比较大,主机挂掉之后从新接管,可能会丢失一部分数据

- 半同步辅助
  - master更新操作写入binlog之后会主动通知slave,slave接收到之后写入ralay log即可应答,master只要收到至少一个ack应答,则会提交事物
  - 可以发现,相比较与异步复制,半同步负责需要依赖至少一个slave将binlog写入relay log,在性能上有所降低,但是可以保证至少有一个从库跟master的数据是一致的,数据的安全性提高
  - 对于数据一致性要求高的场景,可以采用半同步复制的同步策略,比如主库挂掉时,准备接管的那一个从库,对数据的一致性要求比较高
  - 优点: 数据的安全性好
  - 缺点: 性能比异步复制稍低

- 全同步复制
  - 全同步复制必须收到所有从库的ack,才会提交事物
  - 从库的事务提交依赖于后面所有的从库,这样一来性能就会明显下降
  - 优点: 数据一致性最好
  - 缺点: 性能也是最差的



#### mysql innodb的四大日志

- **重做日志（Redo Log）：** 重做日志是 InnoDB 存储引擎中最重要的日志之一。它记录了所有已经提交的事务所做的修改，包括数据的插入、更新和删除操作。这些记录可以在数据库发生崩溃时被用来重新应用，从而恢复到最近一次提交的状态。

- **撤销日志（Undo Log）：** 撤销日志记录了事务中所做的修改的反向操作，用于回滚事务或者恢复到事务未提交之前的状态。它保证了在事务回滚或崩溃恢复时，数据的一致性。

- **数据字典日志（Data Dictionary Log）：** 数据字典日志记录了数据字典中的变更操作，如表结构的修改、创建、删除等。这些变更在 InnoDB 数据字典中的表中存储，确保数据结构的一致性和持久性。

- **二进制日志（Binary Log）：** 二进制日志是 MySQL 的服务器层生成的，而不是 InnoDB 存储引擎专用的。它记录了所有影响数据变更的 SQL 语句，包括 DDL（数据定义语言）和 DML（数据操作语言）语句。这些日志可以用于数据复制和恢复。



#### 数据库的几种锁

- 表级锁:开销小,加锁快;不会出现死锁;锁定颗粒度大,发生锁冲突的概率最高,并发度最低

- 行级锁:开销大,加锁慢;会出现死锁;锁定颗粒度最小,发生锁冲突的概率最低,并发度最高

- 页面锁:开销和加锁时间界于表锁和行锁之间;会出现死锁;锁定粒度界于表锁和行锁之间,并发度一般



#### mvcc

- 是一种基于多版本的并发控制协议，只有在innoDB引擎下存在，mvcc是为了实现事务的隔离性，通过版本号，避免同一数据在不同的事务间的竞争，可以当成多版本的乐观锁，这种乐观锁只有在事务级别是提交读和可重复读中有效。mvcc最大的好处就是，读不加锁，读写不冲突。在读多写少的系统中，读写不冲突极大的提高了系统的并发性

![image-20230829112526049](https://raw.githubusercontent.com/zxinyolo/images/main/202308291125111.png)

#### 最左前匹配原则

MySQL 的最左前缀匹配原则与复合索引有关。它是指在使用复合索引进行查询时，索引的最左边的一列是最重要的，后续列只能在前一列的基础上进行进一步筛选。这个原则非常重要，因为它影响着索引的使用效率和查询性能。

具体来说，最左前缀匹配原则有以下几个关键点：

1. **最左列的匹配：** 在一个复合索引中，只有最左边的列在查询中被使用，索引才能有效地优化查询。如果最左边的列没有被使用，那么索引将无法被利用。
2. **左前缀顺序：** 如果查询中涉及到多个列，那么查询的列必须按照索引列的顺序，从最左边的列开始，依次使用。这就是所谓的左前缀顺序。
3. **跳过列：** 当查询涉及到索引的多列时，如果跳过了前面的列直接使用后面的列，索引的效率会降低。例如，如果复合索引是 (col1, col2, col3)，在查询中只使用了 col3，那么索引只能在 col3 上进行搜索，无法有效地利用 col1 和 col2。

这个原则的目的是最大程度地发挥复合索引的优势，减少数据的扫描和过滤，从而提高查询性能。当你设计表结构和创建索引时，要根据实际查询场景，遵循最左前缀匹配原则来选择合适的索引列和顺序，以优化查询性能。



#### 雪花算法的底层原理及优缺点

适合全局唯一且有序的场景

雪花算法是一种用于生成唯一id的算法，底层原理是在分布式系统中使用一个单独的节点来生成全局唯一id

雪花算法的id号由64位二进制数组成，其中包含5个部分

- 时间戳（占用41位）：记录当前时间戳，精确到毫秒级别。
- 工作机器ID（占用12位）：记录工作机器id，可以部署1024个节点
- 序列号（占用12位）：序列号表示当前毫秒类生成的ID序号，从0开始自增，最多可以生成4096个序号
- 数据中心ID（预留1位）：未来可能会使用的数据中心的ID来扩展机器数量
- 符号位（占用1位）：符号位始终为0，保证ID号始终为正整数

优点是能够快速生成全局唯一的ID号，且不需要依赖第三方库或者服务，另外，雪花算法生成的ID是趋势递增的，有利于数据库索引的优化。

缺点是由于依赖单独的节点来生成ID号，因此存在单点故障的风险，其次如果系统的时钟发生回退，就会导致ID号重复或者生成失败，因此需要对时钟进行同步和监控，最后，雪花算法生成的ID号是连续的，可能会暴漏一些敏感信息（如业务量、时间等），如果需要保护隐私，需要使用加密算法或者其他方式处理



#### uuid当数据库主键的优缺点

适合全局唯一且不需要按顺序排列的场景

- 优点
  - 全局唯一，无论在哪个系统生成，都可以保证全局唯一性
  - 不依赖数据库：uuid不需要依赖数据库就可以生成
  - 隐藏真实业务数据：可以隐藏数据的ID，保护数据的安全性
  - 方便分布式系统：避免ID号相同

- 缺点
  - uuid随机生成的导致数据库索引效率低，查询速度慢
  - 长度大，uuid通常是36位数字的字符串表示，占用更多的存储空间



#### mysql的执行流程是什么

- 连接器：登录校验，权限校验
- 缓存：查看是否命中缓存（之前执行过的sql语句，查询的数据库等信息会做哈希放在一个引用表，查询结果以key-value的方式保存，下次如果hash一样直接返回结果）
- 分析器：根据各个sql的关键字进行解析，生成对应的解析树，根据语法规则对sql进行语法分析及校验，解析成mysql可执行的语句
- 优化器：msyql会生成一条最优的执行计划
- 执行器：执行计划
- 存储引擎：返回数据



#### mysql索引下推(ICP)

ICP(index condition pushdown) 是在mysql5.6上推出的查询优化策略,把本来由server层做的索引条件检查下推给存储引擎来做,以降低回表和访问存储引擎的次数,提高查询效率

查询过程:

- 读取索引记录(不是完整的行记录)
- 判断where条件部分能否用索引中的列来做检查,条件不满足,则处理下一条索引记录
- 条件满足,使用索引中的主键去定位并读取完整的行记录(就是所谓的回表)
- 存储引擎把记录交给server层,server层检测该记录是否满足where条件的其余部分

Tips: 索引下推的目的是为了减少回表次数,也就是要减少IO操作.对于innodb的聚簇索引来说,完整的行记录已经加到缓存区了,索引下推也就没有什么意义了



#### 聚簇索引和非聚簇索引的区别

聚簇索引是堆磁盘上实际数据重新组织以按指定的一个或多个列的值排序的算法

特点是存储数据的顺序和索引顺序一致,一般情况下主键会默认创建聚簇索引,且一张表只允许存在一个聚簇索引(理由: 数据一旦存储,顺序只能有一种)



聚簇索引  的叶子节点就是数据节点

非聚簇索引  的叶子节点仍然是索引索引文件,只是这个索引文件包含指向对应数据块的指针



## Redis

https://juejin.cn/post/6844903982066827277

#### 特点

**Redis中只有网络请求模块和数据操作模块是单线程的。而其他的如持久化存储模块、集群支撑模块等是多线程的。**

采用单线程模式处理请求

- 采用了非阻塞的异步事件处理机制
- 缓存数据都是内存操作IO时间不会太长,单线程可以避免线程上下文切换产生的代价

redis支持持久化,所以它不仅仅可以用作缓存,也可以用作nosql数据库

除了key-value之外,还支持多种数据格式:list,set,zset,hash,string

提供主从同步机制,以及Cluster集群部署能力,能够提供高可用服务



#### Redis是单线程吗?

我们常说的redis单线程指的是:接收客户端请求-> 解析请求-> 进行数据读写等操作-> 返回客户端发生数据,这个过程是由一个线程(主线程)完成的

但是redis程序并不是单线程的,redis启动的时候会启动一个后台线程(BIO):

- redis在2.6版本是会启动两个后台线程,一个用于处理关闭文件,一个用于AOF刷盘的
- Redis 在 4.0 版本之后，新增了一个新的后台线程，用来异步释放 Redis 内存，也就是 lazyfree 线程。例如执行 unlink key / flushdb async / flushall async 等命令，会把这些删除操作交给后台线程来执行，好处是不会导致 Redis 主线程卡顿。因此，当我们要删除一个大 key 的时候，不要使用 del 命令删除，因为 del 是在主线程处理的，这样会导致 Redis 主线程卡顿，因此我们应该使用 unlink 命令来异步删除大key。

之所以 Redis 为「关闭文件、AOF 刷盘、释放内存」这些任务创建单独的线程来处理，是因为这些任务的操作都是很耗时的，如果把这些任务都放在主线程来处理，那么 Redis 主线程就很容易发生阻塞，这样就无法处理后续的请求了。

后台线程相当于一个消费者，生产者把耗时任务丢到任务队列中，消费者（BIO）不停轮询这个队列，拿出任务就去执行对应的方法即可。

![img](https://raw.githubusercontent.com/zxinyolo/images/main/202308281532478.png)

关闭文件、AOF 刷盘、释放内存这三个任务都有各自的任务队列：

- BIO_CLOSE_FILE，关闭文件任务队列：当队列有任务后，后台线程会调用 close(fd) ，将文件关闭；
- BIO_AOF_FSYNC，AOF刷盘任务队列：当 AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封装成一个任务，也放到队列中。当发现队列有任务后，后台线程会调用 fsync(fd)，将 AOF 文件刷盘，
- BIO_LAZY_FREE，lazy free 任务队列：当队列有任务后，后台线程会 free(obj) 释放对象 / free(dict) 删除数据库所有对象 / free(skiplist) 释放跳表对象；



#### Redis单线程模式

Redis 6.0 版本之前的单线模式如下图：

![img](https://raw.githubusercontent.com/zxinyolo/images/main/202308281540584.png)

图中的蓝色部分是一个事件循环，是由主线程负责的，可以看到网络 I/O 和命令处理都是单线程。Redis 初始化的时候，会做下面这几年事情：

- 首先，调用 epoll_create() 创建一个 epoll 对象和调用 socket() 一个服务端 socket
- 然后，调用 bind() 绑定端口和调用 listen() 监听该 socket；
- 然后，将调用 epoll_crt() 将 listen socket 加入到 epoll，同时注册「连接事件」处理函数。

初始化完后，主线程就进入到一个事件循环函数，主要会做以下事情：

首先，先调用处理发送队列函数，看是发送队列里是否有任务，如果有发送任务，则通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发生完，就会注册写事件处理函数，等待 epoll_wait 发现可写后再处理 。

接着，调用 epoll_wait 函数等待事件的到来:

- 如果是连接事件到来，则会调用连接事件处理函数，该函数会做这些事情：调用 accpet 获取已连接的 socket ->  调用 epoll_ctr 将已连接的 socket 加入到 epoll -> 注册「读事件」处理函数；
- 如果是读事件到来，则会调用读事件处理函数，该函数会做这些事情：调用 read 获取客户端发送的数据 -> 解析命令 -> 处理命令 -> 将客户端对象添加到发送队列 -> 将执行结果写到发送缓存区等待发送；
- 如果是写事件到来，则会调用写事件处理函数，该函数会做这些事情：通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发生完，就会继续注册写事件处理函数，等待 epoll_wait 发现可写后再处理 。

以上就是 Redis 单线模式的工作方式。



#### Redis 采用单线程为什么还这么快?

- Redis 的大部分操作都在内存中完成，并且采用了高效的数据结构，因此 Redis 瓶颈可能是机器的内存或者网络带宽，而并非 CPU，既然 CPU 不是瓶颈，那么自然就采用单线程的解决方案了；
- Redis 采用单线程模型可以避免了多线程之间的竞争，省去了多线程切换带来的时间和性能上的开销，而且也不会导致死锁问题。
- Redis 采用了I/O 多路复用机制处理大量的客户端 Socket 请求，IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听 Socket 和已连接 Socket。内核会一直监听这些 Socket 上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。

虽然 Redis 的主要工作（网络 I/O 和执行命令）一直是单线程模型，但是在 Redis 6.0 版本之后，也采用了多个 I/O 线程来处理网络请求，这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上。

所以为了提高网络请求处理的并行度，Redis 6.0 对于网络请求采用多线程来处理。但是对于读写命令，Redis 仍然使用单线程来处理，所以大家不要误解 Redis 有多线程同时执行命令。

Redis 官方表示，Redis 6.0 版本引入的多线程 I/O 特性对性能提升至少是一倍以上。

Redis 6.0 版本支持的 I/O  多线程特性，默认是 I/O 多线程只处理写操作（write client socket），并不会以多线程的方式处理读操作（read client socket）。

#### 数据格式

- string 字符串
  - string类型时redis中最使用的类型,内部的实现是通过SDS(Simple Dynamic String)来存储的.SDS类似java中的ArrayList,可以通过预分配冗余空间的方式来减少内存的频繁分配
  - 这是最简单的类型,就是普通的set和get,做简单的K-V缓存
  - 应用场景: 
    - 缓存功能:
      - string字符串是最常用的数据类型,不仅仅是redis,各个语言多是最基本类希,因此,利用redis作为缓存,配合其它数据作为存储层,利用redis支持高并发的特点,可以大大加快系统的读写速度,以及降低后端数据库的压力
    - 计数器:
      - 许多系统都会使用redis作为系统的实时计数器,可以快速实现计数和查询的功能,而且最终的数据结果可以按照特定的时间落地到数据库或者其他存储介质中进行永久保存
    - 共享用户session
      - 用户重新刷新一次界面,可能需要访问一下数据进行重新登录,或者访问页面缓存Cookie,但是可以利用redis将用户的session集中管理,在这种模式只需要保证Redis的高可用,每次用户Session的更新和获取都可以快速完成,大大提升效率
- Hash
  - 这个类似Map的一种结构,这个一般就是可以将结构化的数据比如一个对象(前提是这个对象没有嵌套其他对象)给缓存在redis里,然后每次读写缓存的时候,可以就操作Hash里的某个字段,但是这个的场景其实还是单一了一些,因为现在很多对象都是比较复杂的,比如你的商品对象可能里面就包含了很多属性,其中也有对象

- List 有序列表
  - 可以通过list存储一些列表的数据结构,类似粉丝列表,文章的评论之类的东西
  - 可以通过Lrange命令,读取某个闭区间内的元素,可以基于List实现分页查询,基于redis实现简单的高性能分页,可以做类似微博那种下拉不断分页的东西,性能高
  - 可以搞个简单的消息队列,从list头添加,从list尾获取
  - 消息队列
    - redis的链表结构,可以轻松实现阻塞队列,可以使用左进右出的命令组成来完成队列的设计,比如数据的生产者可以通过Lpush命令从左边插入数据,多个数据消费者,可以使用BRpop命令阻塞的“抢”列表尾部的数据
    - 文章列表或者数据分页展示的应用
- set 无序集合
  - 可以对一些数据进行快速的全局去重(JVM内存里的HashSet).某个系统部署在多个机器上就可以使用redis进行全局set去重
  - 可以基于set做交集(两个人的好友列表合集-->共同好友),并集,差集
- zset 有序集合
  - 是排序的set,去重但可以排序,写入的时候给一个分数,自动根据分数排序
  - 排行榜 带权重的队列,让更重要的任务优先执行

- 高级用法:
  - Bitmap
    - 位图是支持bit位来存储信息,可以用来实现布隆过滤器
  - HyperLog
    - 供不精确的去重计数功能,比较适合做大规模数据的去重统计,例如统计UV
  - Geospatial
    - 可以用来保存地理位置,并作位置距离计算或者根据半径计算位置等
  - pub/sub
    - 订阅发布功能,可以用作简单的消息队列
  - pipeline
    - 可以批量执行一组指令,一次性返回全部结果,可以减少频繁的请求应答
  - lua
    - redis支持提交lua脚本执行一系列的功能
    - 利用它的原子性可以做秒杀场景
- 事务
  - redis提供的不是严格的事务,redis只保证串行执行命令,并且能保存全部执行,但是执行命令失败时并不会回滚,而是会继续执行下去



#### 持久化

- RDB 快照方式(redis默认的备份方式)

  - 把内存中的数据集以快照形式写入磁盘,实际操作时通过fork子进程执行,采用二进制压缩存储,把整个redis的数据保存在单一文件中,比较适合用来做容灾,冷备
  - 周期性的持久化
  - RDB对redis的性能影响小,因为是fork子进程去做持久化,数据恢复的数据比AOF快
  - 缺点:
    - 快照保存完成之前如果宕机,这段时间的数据将会丢失,另外保存快照时坑可能导致服务器短时间不可用
    - 默认五分钟甚至更久才回生成一次,数据完整性比AOF差

- AOF

  - 以文本日志的形式记录redis处理的每一个写入或删除操作
  - 对日志的写入操作使用的追加模式,有灵活的同步策略,支持每秒同步,每次修改同步和不同步append-only模式没有任何磁盘寻址的开销,所以快,像mysql中的binlog
  - 适合做热备
  - 一秒一次通过一个后台的线程fsync操作,那最多丢一秒的数据
  - 通过一个叫非常可读的方式记录的,这样的特性就适合做灾难性数据误删的紧急恢复了(通过flushall清空了所有数据,只要这个时候后台重写还没有发生,立马拷贝一份AOF日志文件,把最后一条flushall命令删除就可以了)
  - 缺点:
    - 相同规模的数据集,AOF文件要大于RDB文件,AOF在运行效率上往往慢于RDB

  两种机制都开启的时候,redis在重启的时候会默认使用AOF去重新构建数据,因为AOF的数据是比RDB更完整的

  

#### 高可用

​	支持主从同步,提供Cluster集群部署模式,通过sentinel哨兵来监控redis主服务器的状态.当主挂掉时,在从节点中根据一定策略选出新主,并调整其他从slaveof到新主

策略

​	slave的priority设置的越低,优先级越高

​	同等情况下,slave复制的数据越多优先级越高

​	相同条件下runid越小越容易被选中

在redis集群中,sentinel也会进行多实例部署,sentinel之间通过Raft协议来保证自身的高可用

Redis Cluster使用分片机制,在内部分为16384个slot插槽,分布在所有master节点上,每个master节点负责一部分slot.数据操作时按key做CRC16来计算在哪个slot,由哪个master进行处理.数据的冗余是通过slave节点来保障



#### 哨兵

哨兵必须用三个实例去保证自己的健壮性,哨兵 + 主从 并不能保证数据不丢失,但是可以保证集群的高可用

两个哨兵

​	M1   R1

​	S1    S2

​	master宕机了s1和s2两个哨兵只要有一个认为你宕机了就切换了,并且会举出一个哨兵去执行故障,但是这个时候也需要大多数哨兵都是运行的.问题M1宕机了,S1没挂那其实是OK的,但是整个机器挂了呢,哨兵就只剩下S2了,没有哨兵去允许故障转移了,虽然另外一个机器上还有R1,但是故障转移就是不执行

​	M1   S1

R2-S2    R3-S3

​	M1挂了,S2 和 S3 举一个出来执行故障转移

哨兵组件的主要功能:

​	集群监控:负责监控redis master 和 slave 进程是否正常工作

​	消息通知:如果某个Redis实例有故障,那么哨兵负责发送消息作为报警通知给管理员

​	故障转移:如果master node 挂掉了,会自动转移到 slave node上

​	配置中心:如果故障转移发生了,通知client客户端新的master地址	



#### 主从

单机QPS是有上限的,而且redis的特性就是必须支撑读高并发的

master机器写,数据同步给其他slave机器,slave拿去读

扩容的时候还可以轻松实现水平扩容



#### 失效机制

redis的key可以设置过期时间,过期后redis采用主动和被动结合的失效机制,一个是和Memcahe一样在访问时触发被动删除,另一种是定期的主动删除

定期 + 惰性 + 内存淘汰



#### 缓存更新方式

缓存的数据在数据源发生变更时需要对缓存进行更新,数据源可能是DB,也可能是远程服务.更新的方式可以是主动更新.数据源是DB时,可以在更新完DB后就直接更新缓存

当数据源是远程服务时,可能无法及时主动感知数据变更,这种情况下一般会选择对缓存数据设置失效期,也就是数据不一致的最大容忍时间

这种场景下,可以选择失效更新,key不存在或失效时先请求数据源获取最新数据,然后再次缓存,并更新失效期

​	问题

​		如果依赖的远程服务在更新时出现异常,则会导致数据不可用

​	办法

​		异步更新,就是当失效时先不清除数据,继续使用旧的数据,然后由异步线程去执行更新任务.这样就避免了失效瞬间的空窗期

​		纯异步更新,定时对数据进行分批更新



#### 数据不一致

只要使用缓存就要考虑如何面对这个问题

缓存不一致的原因:  一般是主动更新失败,例如更新DB后,更新redis因为网络原因请求超时,或者异步更新失败

解决办法:	如果服务对耗时不是特别敏感可以增加重试;如果服务对耗时敏感可以通过异步补偿任务来处理失败的更新,或者短期的数据不一致不会影响业务,那么只要下次更新时可以成功,能保证最终一致性就可以



#### 缓存穿透

原因

​	外部的恶意攻击,例如对用户信息进行了缓存,但恶意攻击者使用不存在的用户id频繁请求接口,导致查询缓存不命中,然后穿透DB查询依然不命中.这时候会有大量请求穿透缓存访问到DB

办法

​	对不存在的用户,在缓存中保存一个空对象进行标记,防止相同ID再次访问DB.不过有时这个方法并不能很好解决问题,可能导致缓存中存储大量无用数据

​	使用BloomFilter过滤器,BloomFilter的特点是存在性检测,如果BloomFilter中不存在,那么数据一定不存在;如果BloomFilter中存在,实际数据也有可能不会存在,非常适合解决这类问题



#### 缓存击穿

原因

​	某个热点数据失效时,大量针对这个数据的请求会穿透到数据源

办法

​	可以使用互斥锁更新,保证同一个进程中针对同一个数据不会并发请求到DB,减小DB压力

​	使用随机退避方式,失效时随机sleep一个很短的时间,再次查询,如果失败在执行更新

​	针对多个热点key同时失效的问题,可以缓存时使用固定时间加上一个小的随机数,避免大量热点key同一时刻失效



#### 缓存雪崩

原因

​	缓存挂掉,这是所有的请求都会穿透到DB

办法

​	使用快速失败的熔断策略,减少DB瞬间压力

​	使用主从模式和集群模式来尽量保证缓存服务的高可用



## 项目

#### Redis在项目过程中做了哪些事?

#### Celery定时机制是什么

#### 项目遇到过什么性能瓶颈?

#### 组内分工

#### 解耦体现在什么方面

#### 项目做了什么模块

#### 项目介绍,用了什么技术栈

#### 单元测试覆盖率

#### 百万级别的数据如何高效存储

#### celery 单节点怎样保证定时任务创建broker的可靠性

#### mysql inner的数据结构

#### python启动程序后的资源分配情况

